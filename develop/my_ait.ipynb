{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "# AIT Development notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## notebook of structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "| #  | Name                                               | cells | for_dev | edit               | description                                                                |\n",
    "|----|----------------------------------------------------|-------|---------|--------------------|----------------------------------------------------------------------------|\n",
    "| 1  | [Environment detection](##1-Environment-detection) | 1     | No      | uneditable         | detect whether the notebook are invoked for packaging or in production     |\n",
    "| 2  | [Preparing AIT SDK](##2-Preparing-AIT-SDK)         | 1     | Yes     | uneditable         | download and install AIT SDK                                               |\n",
    "| 3  | [Dependency Management](##3-Dependency-Management) | 3     | Yes     | required(cell #2)  | generate requirements.txt for Docker container                             |\n",
    "| 4  | [Importing Libraries](##4-Importing-Libraries)     | 2     | Yes     | required(cell #1)  | import required libraries                                                  |\n",
    "| 5  | [Manifest Generation](##5-Manifest-Generation)     | 1     | Yes     | required           | generate AIT Manifest                                                      |\n",
    "| 6  | [Prepare for the Input](##6-Prepare-for-the-Input) | 1     | Yes     | required           | generate AIT Input JSON (inventory mapper)                                 |\n",
    "| 7  | [Initialization](##7-Initialization)               | 1     | No      | uneditable         | initialization for AIT execution                                           |\n",
    "| 8  | [Function definitions](##8-Function-definitions)   | N     | No      | required           | define functions invoked from Main area.<br> also define output functions. |\n",
    "| 9  | [Main Algorithms](##9-Main-Algorithms)             | 1     | No      | required           | area for main algorithms of an AIT                                         |\n",
    "| 10 | [Entry point](##10-Entry-point)                    | 1     | No      | uneditable         | an entry point where Qunomon invoke this AIT from here                     |\n",
    "| 11 | [License](##11-License)                            | 1     | Yes     | required           | generate license information                                               |\n",
    "| 12 | [Deployment](##12-Deployment)                      | 1     | Yes     | uneditable         | convert this notebook to the python file for packaging purpose             |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## notebook template revision history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "1.0.1 2020/10/21\n",
    "\n",
    "* add revision history\n",
    "* separate `create requirements and pip install` editable and noeditable\n",
    "* separate `import` editable and noeditable\n",
    "\n",
    "1.0.0 2020/10/12\n",
    "\n",
    "* new cerarion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## body"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### #1 Environment detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "[uneditable]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Determine whether to start AIT or jupyter by startup argument\n",
    "import sys\n",
    "is_ait_launch = (len(sys.argv) == 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### #2 Preparing AIT SDK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "[uneditable]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "if not is_ait_launch:\n",
    "    # get ait-sdk file name\n",
    "    from pathlib import Path\n",
    "    from glob import glob\n",
    "    import re\n",
    "    import os\n",
    "\n",
    "    current_dir = %pwd\n",
    "\n",
    "    ait_sdk_path = \"./ait_sdk-*-py3-none-any.whl\"\n",
    "    ait_sdk_list = glob(ait_sdk_path)\n",
    "    ait_sdk_name = os.path.basename(ait_sdk_list[-1])\n",
    "\n",
    "    # install ait-sdk\n",
    "    !pip install -q --upgrade pip\n",
    "    !pip install -q --no-deps --force-reinstall ./$ait_sdk_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### #3 Dependency Management"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "#### #3-1 [uneditable]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "if not is_ait_launch:\n",
    "    from ait_sdk.common.files.ait_requirements_generator import AITRequirementsGenerator\n",
    "    requirements_generator = AITRequirementsGenerator()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "#### #3-2 [required]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not is_ait_launch:\n",
    "     requirements_generator.add_package('tensorflow', '2.16.2')\n",
    "     requirements_generator.add_package('numpy', '1.26.4')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "#### #3-3 [uneditable]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "if not is_ait_launch:\n",
    "    requirements_generator.add_package(f'./{ait_sdk_name}')\n",
    "    requirements_path = requirements_generator.create_requirements(current_dir)\n",
    "\n",
    "    !pip install -q -r $requirements_path "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### #4 Importing Libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "#### #4-1 [required]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import time\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import wp_geb.utils as utl\n",
    "import wp_geb.search as sch\n",
    "import wp_geb.measure as msr\n",
    "import wp_geb.estimate as est\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "#### #4-2 [uneditable]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# must use modules\n",
    "from os import path\n",
    "import shutil  # do not remove\n",
    "from ait_sdk.common.files.ait_input import AITInput  # do not remove\n",
    "from ait_sdk.common.files.ait_output import AITOutput  # do not remove\n",
    "from ait_sdk.common.files.ait_manifest import AITManifest  # do not remove\n",
    "from ait_sdk.develop.ait_path_helper import AITPathHelper  # do not remove\n",
    "from ait_sdk.utils.logging import get_logger, log, get_log_path  # do not remove\n",
    "from ait_sdk.develop.annotation import measures, resources, downloads, ait_main  # do not remove\n",
    "# must use modules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### #5 Manifest Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "[required]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not is_ait_launch:\n",
    "     from ait_sdk.common.files.ait_manifest_generator import AITManifestGenerator\n",
    "     manifest_genenerator = AITManifestGenerator(current_dir)\n",
    "     manifest_genenerator.set_ait_name('eval_model_classifier_image_stability_wpgeb')\n",
    "     manifest_genenerator.set_ait_description('This AIT is a simplified version of WP-GEB-Estimator-2, designed to estimates weight-perturbed generalization bounds. It can be used for evaluating the statistically certified stability of neural classifiers. The manual can be downloaded from the web-site of WP-GEB-Estimator (https://staff.aist.go.jp/y-isobe/wp-geb-estimator/).')\n",
    "     manifest_genenerator.set_ait_source_repository('https://github.com/aistairc/eval_model_classifier_image_stability_wpgeb')\n",
    "     manifest_genenerator.set_ait_version('0.1')\n",
    "     manifest_genenerator.add_ait_licenses('Apache License Version 2.0')\n",
    "     manifest_genenerator.add_ait_keywords('generalization risk')\n",
    "     manifest_genenerator.set_ait_quality('https://ait-hub.pj.aist.go.jp/ait-hub/api/0.0.1/qualityDimensions/機械学習品質マネジメントガイドライン第三版/C-2機械学習モデルの安定性')\n",
    "     inventory_requirement_data = manifest_genenerator.format_ait_inventory_requirement(format_=['gz'])\n",
    "     inventory_requirement_model = manifest_genenerator.format_ait_inventory_requirement(format_=['h5'])\n",
    "     manifest_genenerator.add_ait_inventories(name='images_gz', \n",
    "                                              type_='dataset', \n",
    "                                              description='The list (gzipped flat numpy array) of the images in the test dataset. Note: the image data are directly input to the classifier without any normalization. For example, if a classifier is trained by the normalized gray scale (0, 1) instead of (0, 255) in MNIST, then the image file with (already normalized) gray scale (0, 1) has to be loaded in this AIT.',\n",
    "                                              requirement=inventory_requirement_data)\n",
    "     manifest_genenerator.add_ait_inventories(name='labels_gz', \n",
    "                                              type_='dataset',\n",
    "                                              description='The list (gzipped flat numpy array) of the labels in the test dataset',\n",
    "                                              requirement=inventory_requirement_data)\n",
    "     manifest_genenerator.add_ait_inventories(name='model_h5', \n",
    "                                              type_='model',\n",
    "                                              description='The evaluated model (classifier) by the test dataset',\n",
    "                                              requirement=inventory_requirement_model)\n",
    "     manifest_genenerator.add_ait_parameters(name='dataset_size',\n",
    "                                             type_='int', \n",
    "                                             description='dataset size used for testing', \n",
    "                                             default_val='1000')\n",
    "     manifest_genenerator.add_ait_parameters(name='image_width',\n",
    "                                             type_='int', \n",
    "                                             description='width of each image', \n",
    "                                             default_val='32')\n",
    "     manifest_genenerator.add_ait_parameters(name='image_height',\n",
    "                                             type_='int',\n",
    "                                             description='height of each image', \n",
    "                                             default_val='32')\n",
    "     manifest_genenerator.add_ait_parameters(name='color_size',\n",
    "                                             type_='int',\n",
    "                                             description='color size of each image', \n",
    "                                             default_val='3')\n",
    "     manifest_genenerator.add_ait_parameters(name='search_batch_size',\n",
    "                                             type_='int',\n",
    "                                             description='batch size for parallel search', \n",
    "                                             default_val='10')\n",
    "     manifest_genenerator.add_ait_parameters(name='prediction_batch_size',\n",
    "                                             type_='int',\n",
    "                                             description='batch size for parallel prediction', \n",
    "                                             default_val='200')\n",
    "     manifest_genenerator.add_ait_parameters(name='perturb_ratio',\n",
    "                                             type_='float',\n",
    "                                             description='ratio of maximum weight-perturbation to weight', \n",
    "                                             default_val='0.001')\n",
    "     manifest_genenerator.add_ait_parameters(name='skip_search',\n",
    "                                             type_='int',\n",
    "                                             description='1 if gradient-based search is skipped (0 otherwise)',\n",
    "                                             default_val='0')\n",
    "     manifest_genenerator.add_ait_parameters(name='err_threshold',\n",
    "                                             type_='float',\n",
    "                                             description='acceptable error threshold',\n",
    "                                             default_val='0.01')\n",
    "     manifest_genenerator.add_ait_parameters(name='confidence',\n",
    "                                             type_='float',\n",
    "                                             description='confidence of generalization bounds',\n",
    "                                             default_val='0.9')\n",
    "     manifest_genenerator.add_ait_measures(name='gen_err_ub', \n",
    "                                           type_='float', \n",
    "                                           description='generalization error upper bound (no weight-perturbation)',\n",
    "                                           structure='single',\n",
    "                                           min='0',\n",
    "                                           max='1')\n",
    "     manifest_genenerator.add_ait_measures(name='test_err', \n",
    "                                           type_='float', \n",
    "                                           description='test error (no weight-perturbation)', \n",
    "                                           structure='single',\n",
    "                                           min='0',\n",
    "                                           max='1')\n",
    "     manifest_genenerator.add_ait_measures(name='wp_gen_risk_ub', \n",
    "                                           type_='float', \n",
    "                                           description='weight-perturbed generalization risk upper bound', \n",
    "                                           structure='single',\n",
    "                                           min='0',\n",
    "                                           max='1')\n",
    "     manifest_genenerator.add_ait_measures(name='wp_test_risk_ub', \n",
    "                                           type_='float', \n",
    "                                           description='weight-perturbed test risk upper bound', \n",
    "                                           structure='single',\n",
    "                                           min='0',\n",
    "                                           max='1')\n",
    "     manifest_genenerator.add_ait_measures(name='wp_gen_err_ub', \n",
    "                                           type_='float', \n",
    "                                           description='weight-perturbed generalization error upper bound', \n",
    "                                           structure='single',\n",
    "                                           min='0',\n",
    "                                           max='1')\n",
    "     manifest_genenerator.add_ait_measures(name='wp_test_err_ub', \n",
    "                                           type_='float', \n",
    "                                           description='weight-perturbed test error upper bound', \n",
    "                                           structure='single',\n",
    "                                           min='0',\n",
    "                                           max='1')\n",
    "     manifest_genenerator.add_ait_resources(name='estimation_result',  \n",
    "                                           type_='text', \n",
    "                                           description='estimation result')\n",
    "     manifest_genenerator.add_ait_resources(name='input_output_data',\n",
    "                                           type_='table', \n",
    "                                           description='input data and output data')\n",
    "     manifest_genenerator.add_ait_downloads(name='Log', \n",
    "                                            description='AIT log')\n",
    "     manifest_path = manifest_genenerator.write()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### #6 Prepare for the Input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "[required]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not is_ait_launch:\n",
    "     from ait_sdk.common.files.ait_input_generator import AITInputGenerator\n",
    "     input_generator = AITInputGenerator(manifest_path)\n",
    "     input_generator.add_ait_inventories(name='images_gz',\n",
    "                                         value='datasets/cifar10/images_test_1000.gz')\n",
    "     input_generator.add_ait_inventories(name='labels_gz',\n",
    "                                         value='datasets/cifar10/labels_test_1000.gz')\n",
    "     input_generator.add_ait_inventories(name='model_h5',\n",
    "                                         value='models/cifar10/cnn_m.h5')\n",
    "     input_generator.write()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### #7 Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "[uneditable]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "logger = get_logger()\n",
    "\n",
    "ait_manifest = AITManifest()\n",
    "ait_input = AITInput(ait_manifest)\n",
    "ait_output = AITOutput(ait_manifest)\n",
    "\n",
    "if is_ait_launch:\n",
    "    # launch from AIT\n",
    "    current_dir = path.dirname(path.abspath(__file__))\n",
    "    path_helper = AITPathHelper(argv=sys.argv, ait_input=ait_input, ait_manifest=ait_manifest, entry_point_dir=current_dir)\n",
    "else:\n",
    "    # launch from jupyter notebook\n",
    "    # ait.input.json make in input_dir\n",
    "    input_dir = '/usr/local/qai/mnt/ip/job_args/1/1'\n",
    "    current_dir = %pwd\n",
    "    path_helper = AITPathHelper(argv=['', input_dir], ait_input=ait_input, ait_manifest=ait_manifest, entry_point_dir=current_dir)\n",
    "\n",
    "ait_input.read_json(path_helper.get_input_file_path())\n",
    "ait_manifest.read_json(path_helper.get_manifest_file_path())\n",
    "\n",
    "### do not edit cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### #8 Function definitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "[required]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "@log(logger)\n",
    "@measures(ait_output, 'gen_err_ub')\n",
    "def out_gen_err_ub(gen_err_ub):\n",
    "    return gen_err_ub\n",
    "\n",
    "@log(logger)\n",
    "@measures(ait_output, 'test_err')\n",
    "def out_test_err(test_err):\n",
    "    return test_err\n",
    "\n",
    "@log(logger)\n",
    "@measures(ait_output, 'wp_gen_risk_ub')\n",
    "def out_wp_gen_risk_ub(wp_gen_risk_ub):\n",
    "    return wp_gen_risk_ub\n",
    "\n",
    "@log(logger)\n",
    "@measures(ait_output, 'wp_test_risk_ub')\n",
    "def out_wp_test_risk_ub(wp_test_risk_ub):\n",
    "    return wp_test_risk_ub\n",
    "\n",
    "@log(logger)\n",
    "@measures(ait_output, 'wp_gen_err_ub')\n",
    "def out_wp_gen_err_ub(wp_gen_err_ub):\n",
    "    return wp_gen_err_ub\n",
    "\n",
    "@log(logger)\n",
    "@measures(ait_output, 'wp_test_err_ub')\n",
    "def out_wp_test_err_ub(wp_test_err_ub):\n",
    "    return wp_test_err_ub\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "@log(logger)\n",
    "@resources(ait_output, path_helper, 'estimation_result', 'estimation_result.txt')\n",
    "def save_estimation_result(txt, file_path: str=None) -> str:\n",
    "    with open(file_path, 'w') as f:\n",
    "        f.write(txt)\n",
    "\n",
    "@log(logger)\n",
    "@resources(ait_output, path_helper, 'input_output_data', 'input_output_data.csv')\n",
    "def save_input_output_data(csv, file_path: str=None) -> str:\n",
    "    with open(file_path, 'w') as f:\n",
    "        f.write(csv)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "## sample ##\n",
    "@log(logger)\n",
    "@downloads(ait_output, path_helper, 'Log', 'ait.log')\n",
    "def move_log(file_path: str=None) -> str:\n",
    "    shutil.move(get_log_path(), file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### #9 Main Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "[required]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "@log(logger)\n",
    "@ait_main(ait_output, path_helper, is_ait_launch)\n",
    "def main() -> None:\n",
    "\n",
    "    # constant\n",
    "    perturb_batch_norm = 0\n",
    "    batch_normalization_name = 'BatchNormalization'\n",
    "    not_available = 'N/A'\n",
    "    delta0_ratio = 0.5\n",
    "\n",
    "    # parameters\n",
    "    param_dataset_size = ait_input.get_method_param_value('dataset_size')\n",
    "    param_image_width = ait_input.get_method_param_value('image_width')\n",
    "    param_image_height = ait_input.get_method_param_value('image_height')\n",
    "    param_color_size = ait_input.get_method_param_value('color_size')\n",
    "    param_search_batch_size = ait_input.get_method_param_value('search_batch_size')\n",
    "    param_prediction_batch_size = ait_input.get_method_param_value('prediction_batch_size')\n",
    "    param_perturb_ratio = ait_input.get_method_param_value('perturb_ratio')\n",
    "    param_skip_search = ait_input.get_method_param_value('skip_search')\n",
    "    param_err_threshold = ait_input.get_method_param_value('err_threshold')\n",
    "    param_confidence = ait_input.get_method_param_value('confidence')\n",
    "    \n",
    "    delta = 1 - param_confidence\n",
    "\n",
    "    # input\n",
    "    images_file_name = ait_input.get_inventory_path('images_gz')\n",
    "    labels_file_name = ait_input.get_inventory_path('labels_gz')\n",
    "    model_file_name = ait_input.get_inventory_path('model_h5')\n",
    "\n",
    "    # --- load dataset ---\n",
    "    images, labels = utl.load_dataset_list(\n",
    "        labels_file_name, images_file_name,\n",
    "        param_image_width, param_image_height, param_color_size,\n",
    "        param_dataset_size)\n",
    "    dataset_size = labels.shape[0]\n",
    "\n",
    "    # --- load model ---\n",
    "    print('Load model: ', model_file_name)\n",
    "    model = tf.keras.models.load_model(model_file_name)\n",
    "    # model.summary()\n",
    "    if perturb_batch_norm == 0:\n",
    "        model = utl.set_non_trainable_layer(model, batch_normalization_name)\n",
    "    p_params_size = utl.model_trainable_params_size(model)\n",
    "\n",
    "    time1 = time.time()\n",
    "\n",
    "    dataset_list = utl.split_nary(images, labels, param_prediction_batch_size)\n",
    "    test_err, tmp_id_list = msr.error_evaluation_batch(model, dataset_list)\n",
    "\n",
    "    # --- gradient-based search for risky data ---\n",
    "    if param_skip_search == 1 or param_perturb_ratio == 0:\n",
    "        err_id_list = []\n",
    "        err_num_search = 0\n",
    "        print('Gradient-based search: skipped')\n",
    "    else:\n",
    "        print('Gradient-based search')\n",
    "        err_id_list = sch.search(\n",
    "            model, images, labels, param_search_batch_size, param_perturb_ratio)\n",
    "        err_num_search = len(err_id_list)\n",
    "        # info_str = 'Detected risky data: {:d}/{:d}'.format(\n",
    "        #     err_num_search, dataset_size)\n",
    "        # print(info_str)\n",
    "        # print('err_id_list = ', err_id_list)\n",
    "\n",
    "    # --- measure errs with random weight perturbations ---\n",
    "    print('Random perturbation')\n",
    "            \n",
    "    if len(err_id_list) > 0:\n",
    "        images = np.delete(images, np.array(err_id_list), axis=0)\n",
    "        labels = np.delete(labels, np.array(err_id_list))\n",
    "\n",
    "    nd_dataset_size = labels.shape[0]\n",
    "\n",
    "    if nd_dataset_size == 0 or param_perturb_ratio == 0:\n",
    "        err_num_random = 0\n",
    "        practical_err_thr = 0\n",
    "        perturb_sample_size = 0\n",
    "        wp_test_err = test_err\n",
    "\n",
    "    else:\n",
    "        delta0 = delta * delta0_ratio\n",
    "        delta1 = delta0 / nd_dataset_size\n",
    "        perturb_sample_size = math.ceil(math.log(delta1, 1 - param_err_threshold))\n",
    "        practical_err_thr = 1 - math.exp(-math.log(1 / delta1) / perturb_sample_size)\n",
    "\n",
    "        err_count = msr.measure(\n",
    "            model, images, labels, param_prediction_batch_size,\n",
    "            param_perturb_ratio, perturb_sample_size)\n",
    "\n",
    "        wp_test_err = np.sum(err_count) / (nd_dataset_size * perturb_sample_size)\n",
    "\n",
    "        err_flag = (err_count > 0).astype(int)\n",
    "        err_num_random = np.sum(err_flag)\n",
    "\n",
    "    # --- generalization ---\n",
    "    print('Generalization')\n",
    "    \n",
    "    delta0 = delta * delta0_ratio\n",
    "\n",
    "    (gen_err_ub,\n",
    "     wp_gen_risk_ub, wp_test_risk_ub, conf_risk, conf0_risk,\n",
    "     non_det_rate_ub, gen_err_thr_ub,\n",
    "     wp_gen_err_ub, wp_test_err_ub, conf_err, conf0_err,\n",
    "     avl_err) = est.estimate(\n",
    "        dataset_size, err_num_search, err_num_random, param_perturb_ratio,\n",
    "        test_err, delta, delta0, param_err_threshold, perturb_sample_size)\n",
    "\n",
    "    time2 = time.time()\n",
    "    elapsed_time = time2 - time1\n",
    "\n",
    "    # make estimation result message\n",
    "    info_str = ''\n",
    "    info_str += 'Model file: ' + model_file_name + '\\n'\n",
    "    info_str += 'Image file: ' + images_file_name + '\\n'\n",
    "    info_str += 'Label file: ' + labels_file_name + '\\n\\n'\n",
    "\n",
    "    info_str += 'Normal Test (without weight-perturbation):\\n'\n",
    "    info_str += '  Generalization error bound: '\n",
    "    info_str += '{:.2f}% (Conf: {:g}%)\\n'.format(\n",
    "        gen_err_ub * 100, (1 - delta) * 100)\n",
    "    info_str += '  Test error: '\n",
    "    info_str += '{:.2f}%\\n'.format(\n",
    "        test_err * 100)\n",
    "\n",
    "    if param_perturb_ratio > 0:\n",
    "        info_str += 'Test with weight-perturbation (perturbation ratio: {:g}):\\n'.format(\n",
    "            param_perturb_ratio)\n",
    "\n",
    "        if param_skip_search == 0:\n",
    "            info_str += '  Gradient-based search\\n'\n",
    "            info_str += '    Detected risky data: {:d}/{:d}\\n'.format(\n",
    "                err_num_search, dataset_size)\n",
    "            info_str += '    Generalization non-detection rate bound: {:.2f}% (Conf: {:g}%)\\n'.format(\n",
    "                non_det_rate_ub * 100, (1 - delta) * 100)\n",
    "\n",
    "        info_str += '  Random perturbation (sample size: {:d})\\n'.format(\n",
    "            perturb_sample_size)\n",
    "\n",
    "        # gen-ub (risk adapt)\n",
    "        if err_num_search == 0:\n",
    "            info_str += '    Estimated risk (without search):\\n'\n",
    "        else:\n",
    "            info_str += '    Estimated risk (with search):\\n'\n",
    "        info_str += '      Perturbed generalization risk bound: '\n",
    "        info_str += '{:.2f}% (Conf: {:g}%)\\n'.format(\n",
    "            wp_gen_risk_ub * 100, conf_risk * 100)\n",
    "        info_str += '      Perturbed test risk bound: '\n",
    "        info_str += '{:.2f}% (Conf: {:g}%)\\n'.format(\n",
    "            wp_test_risk_ub * 100, conf0_risk * 100)\n",
    "        info_str += '        Generalization acceptable error threshold bound: '\n",
    "        info_str += '{:g}% (Conf: {:g}%)\\n'.format(\n",
    "            gen_err_thr_ub * 100, conf_risk * 100)\n",
    "        info_str += '        Individual acceptable error threshold: '\n",
    "        info_str += '{:g}% (Practical: {:.4f}%)\\n'.format(\n",
    "            param_err_threshold * 100, practical_err_thr * 100)\n",
    "\n",
    "        if avl_err:\n",
    "            # gen-ub (err)\n",
    "            info_str += '    Estimated error:\\n'\n",
    "            info_str += '      Perturbed generalization error bound: '\n",
    "            info_str += '{:.2f}% (Conf: {:g}%)\\n'.format(\n",
    "                wp_gen_err_ub * 100, conf_err * 100)\n",
    "            info_str += '      Perturbed test error bound: '\n",
    "            info_str += '{:.2f}% (Conf: {:g}%)\\n'.format(\n",
    "                wp_test_err_ub * 100, conf0_err * 100)\n",
    "            info_str += '        Sample-perturbed test error: '\n",
    "            info_str += '{:.2f}% \\n'.format(wp_test_err * 100)\n",
    "\n",
    "    info_str += '\\nThe meaning of the results is as follows:\\n\\n'\n",
    "    info_str += ('Test error, which is misclassification rate, of the neural classifier '\n",
    "                 'given by Model file ')\n",
    "    info_str += 'is {:.2f}% '.format(test_err * 100)\n",
    "    info_str += 'for the dataset given by Image file and Label file. '\n",
    "    info_str += ('Here, it is assumed that the dataset is sampled '\n",
    "                 'according to an independent and identical distribution (i.i.d). ')\n",
    "    info_str += ('The generalization error, '\n",
    "                 'which is the expected value of error '\n",
    "                 'for any input-image (i.e., including unseen images) sampled '\n",
    "                 'according to the distribution, '\n",
    "                 'is less than {:.2f}% at least {:g}% confidence.\\n\\n').format(\n",
    "        gen_err_ub * 100, (1 - delta) * 100)\n",
    "\n",
    "    info_str += 'Hereafter, the ratio of weight-perturbation to weight is less than {:g}. '.format(\n",
    "            param_perturb_ratio)\n",
    "    if avl_err:\n",
    "        info_str += ('The weight-perturbed generalization error for any image and '\n",
    "                     'any weight-perturbation '\n",
    "                     'is less than {:.2f}% at least {:g}% confidence. ').format(\n",
    "            wp_gen_err_ub * 100, conf_err * 100)\n",
    "    info_str += ('The weight-perturbed generalization risk, '\n",
    "                 'which is the expected value of the probability such that '\n",
    "                 'randomly sampled input-image is risky, '\n",
    "                 'is less than {:.2f}% at least {:g}% confidence. ').format(\n",
    "        wp_gen_risk_ub * 100, conf_risk * 100)\n",
    "    info_str += ('Here, an input-image is said to be risky if the misclassification-rate '\n",
    "                 'caused by weight-perturbations for the image exceeds '\n",
    "                 'the acceptable error threshold which is less than {:g}%. ').format(\n",
    "        param_err_threshold * 100)\n",
    "    info_str += ('Generalization acceptance error threshold is the expected value of '\n",
    "                 'the acceptable error threshold for any input-image, and it is less than'\n",
    "                 '{:.2f}% at least {:g}% confidence. \\n\\n').format(\n",
    "        gen_err_thr_ub * 100, conf_risk * 100)\n",
    "\n",
    "    info_str += '(Elapsed time for estimation: {:.1f} [sec])\\n'.format(elapsed_time)\n",
    "\n",
    "    # make input output data (csv)\n",
    "    info_csv = 'variable name, description, value\\n'\n",
    "\n",
    "    info_csv += 'images_file_name, images in the test dataset, ' + str(images_file_name) + '\\n'\n",
    "    info_csv += 'labels_file_name, labels in the test dataset, ' + str(labels_file_name) + '\\n'\n",
    "    info_csv += 'model_file_name, evaluated classifier, ' + str(model_file_name) + '\\n '\n",
    "    \n",
    "    info_csv += 'dataset_size, dataset size used for testing, ' + str(param_dataset_size) + '\\n'\n",
    "    info_csv += 'image_width, width of each image, ' + str(param_image_width) + '\\n'\n",
    "    info_csv += 'image_height, height of each image, ' + str(param_image_height) + '\\n'\n",
    "    info_csv += 'color_size, RGB:3 or Gray:1, ' +  str(param_color_size) + '\\n'\n",
    "    info_csv += 'search_batch_size, batch size for parallel search, ' + str(param_search_batch_size) + '\\n'\n",
    "    info_csv += 'prediction_batch_size, batch size for parallel prediction, ' + str(param_prediction_batch_size)  + '\\n'\n",
    "    info_csv += 'perturb_ratio, ratio of maximum weight-perturbation to weight, ' + str(param_perturb_ratio) + '\\n'\n",
    "    info_csv += 'skip_search, skip search if 1, ' + str(param_skip_search) + '\\n'\n",
    "    info_csv += 'err_threshold, acceptable error threshold, ' + str(param_err_threshold) + '\\n'\n",
    "    info_csv += 'confidence, confidence of generalization bounds, ' + str(param_confidence) + '\\n'\n",
    "\n",
    "    info_csv += 'perturbed_size, the number of perturbed weigh-parameters, ' + str(p_params_size) + '\\n'\n",
    "    info_csv += 'gen_err_ub, generalization error upper bound (no weight-perturbation), ' + str(gen_err_ub) + '\\n'\n",
    "    info_csv += 'test_err, test error (no weight-perturbation), ' + str(test_err) + '\\n'\n",
    "\n",
    "    if param_skip_search == 0:\n",
    "        info_csv += 'err_num_search, the number of detected risky data by search, ' + str(err_num_search) + '\\n'\n",
    "        info_csv += 'non_det_rate_ub, generalization non-detection rate bound, ' + str(non_det_rate_ub) + '\\n'\n",
    "    else:\n",
    "        info_csv += 'err_num_search, the number of detected risky data by search, ' + not_available + '\\n'\n",
    "        info_csv += 'non_det_rate_ub, generalization non-detection rate bound, ' + not_available + '\\n'\n",
    "\n",
    "    info_csv += 'perturb_sample_size, perturbation sample size, ' + str(perturb_sample_size) + '\\n'\n",
    "    info_csv += 'practical_err_thr, practically used threshold instead of err_threshold, ' + str(practical_err_thr) + '\\n'\n",
    "\n",
    "    info_csv += 'wp_gen_risk_ub, weight-perturbed generalization risk upper bound, ' + str(wp_gen_risk_ub) + '\\n'\n",
    "    info_csv += 'wp_test_risk_ub, weight-perturbed test risk upper bound, ' + str(wp_test_risk_ub) + '\\n'\n",
    "    info_csv += 'conf_risk, confidence of wp_gen_risk_ub, ' + str(conf_risk) + '\\n'\n",
    "    info_csv += 'conf0_risk, confidence of wp_test_risk_ub, ' + str(conf0_risk) + '\\n'\n",
    "    info_csv += 'gen_err_thr_ub, generalization acceptance error threshold, ' + str(gen_err_thr_ub) + '\\n'\n",
    "\n",
    "    if avl_err:\n",
    "        info_csv += 'wp_gen_err_ub, weight-perturbed generalization error upper bound, ' + str(wp_gen_err_ub) + '\\n'\n",
    "        info_csv += 'wp_test_err_ub, weight-perturbed test error upper bound, ' + str(wp_test_err_ub) + '\\n'\n",
    "        info_csv += 'conf_err, confidence of wp_gen_err_ub, ' + str(conf_err) + '\\n'\n",
    "        info_csv += 'conf0_err, confidence of wp_test_err_ub, ' + str(conf0_err) + '\\n'\n",
    "    else:\n",
    "        info_csv += 'wp_gen_err_ub, weight-perturbed generalization error upper bound, ' + not_available + '\\n'\n",
    "        info_csv += 'wp_test_err_ub, weight-perturbed test error upper bound, ' + not_available + '\\n'\n",
    "        info_csv += 'conf_err, confidence of wp_gen_err_ub, ' + not_available + '\\n'\n",
    "        info_csv += 'conf0_err, confidence of wp_test_err_ub, ' + not_available + '\\n'\n",
    "\n",
    "    # save\n",
    "    save_estimation_result(info_str)\n",
    "    save_input_output_data(info_csv)\n",
    "    \n",
    "    # measures\n",
    "    out_gen_err_ub(gen_err_ub)\n",
    "    out_test_err(test_err)\n",
    "    out_wp_gen_risk_ub(wp_gen_risk_ub)\n",
    "    out_wp_test_risk_ub(wp_test_risk_ub)\n",
    "    out_wp_gen_err_ub(wp_gen_err_ub)\n",
    "    out_wp_test_err_ub(wp_test_err_ub)\n",
    "    \n",
    "    move_log()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### #10 Entry point"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "[uneditable]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load labels:  /usr/local/qai/inventory/datasets/cifar10/labels_test_1000.gz\n",
      "Load images:  /usr/local/qai/inventory/datasets/cifar10/images_test_1000.gz\n",
      "Load model:  /usr/local/qai/inventory/models/cifar10/cnn_m.h5\n",
      "Gradient-based search\n",
      "Random perturbation\n",
      "Generalization\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### #11 License"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "[required]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "ait_owner='AIST'\n",
    "ait_creation_year='2025'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### #12 Deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "[uneditable] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "if not is_ait_launch:\n",
    "    from ait_sdk.deploy import prepare_deploy\n",
    "    from ait_sdk.license.license_generator import LicenseGenerator\n",
    "    \n",
    "    current_dir = %pwd\n",
    "    prepare_deploy(ait_sdk_name, current_dir, requirements_path)\n",
    "    \n",
    "    # output License.txt\n",
    "    license_generator = LicenseGenerator()\n",
    "    license_generator.write('../top_dir/LICENSE.txt', ait_creation_year, ait_owner)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Edit Metadata",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "cc00c6a56d87bd8bd7773e730c60ddfdb8804da6b7537df09499efbcf81630f7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
